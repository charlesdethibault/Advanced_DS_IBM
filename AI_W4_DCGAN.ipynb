{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "MIT License\n\nCopyright (c) 2017 Erik Linder-Nor\u00e9n\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE."}, {"metadata": {}, "cell_type": "code", "source": "#Please make sure you have at least TensorFlow version 1.12 installed, if not please uncomment and use the \n# pip command below to upgrade. When in a jupyter environment (especially IBM Watson Studio),\n# please don't forget to restart the kernel\nimport tensorflow as tf\ntf.__version__", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "!pip install --upgrade tensorflow", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from __future__ import print_function, division\n\nfrom keras.datasets import mnist\nfrom keras.layers import Input, Dense, Reshape, Flatten, Dropout\nfrom keras.layers import BatchNormalization, Activation, ZeroPadding2D\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.layers.convolutional import UpSampling2D, Conv2D\nfrom keras.models import Sequential, Model\nfrom keras.optimizers import Adam\n\nimport matplotlib.pyplot as plt\n\nimport sys\n\nimport numpy as np", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "img_rows = 28\nimg_cols = 28\nchannels = 1\nlatent_dim = 100\nimg_shape = (img_rows, img_cols, channels)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "def build_generator():\n\n    model = Sequential()\n\n    model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=latent_dim))\n    model.add(Reshape((7, 7, 128)))\n    model.add(UpSampling2D())\n    model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Activation(\"relu\"))\n    model.add(UpSampling2D())\n    model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Activation(\"relu\"))\n    model.add(Conv2D(channels, kernel_size=3, padding=\"same\"))\n    model.add(Activation(\"tanh\"))\n\n    model.summary()\n\n    noise = Input(shape=(latent_dim,))\n    img = model(noise)\n\n    return Model(noise, img)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "def build_discriminator():\n\n    model = Sequential()\n\n    model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=img_shape, padding=\"same\"))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dropout(0.25))\n    model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n    model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dropout(0.25))\n    model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dropout(0.25))\n    model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dropout(0.25))\n    model.add(Flatten())\n    model.add(Dense(1, activation='sigmoid'))\n\n    model.summary()\n\n    img = Input(shape=img_shape)\n    validity = model(img)\n\n    return Model(img, validity)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "\n\noptimizer = Adam(0.0002, 0.5)\n\n# Build and compile the discriminator\ndiscriminator = build_discriminator()\ndiscriminator.compile(loss='binary_crossentropy',\n    optimizer=optimizer,\n    metrics=['accuracy'])\n\n# Build the generator\ngenerator = build_generator()\n\n# The generator takes noise as input and generates imgs\nz = Input(shape=(latent_dim,))\nimg = generator(z)\n\n# For the combined model we will only train the generator\ndiscriminator.trainable = False\n\n# The discriminator takes generated images as input and determines validity\nvalid = discriminator(img)\n\n# The combined model  (stacked generator and discriminator)\n# Trains the generator to fool the discriminator\ncombined = Model(z, valid)\ncombined.compile(loss='binary_crossentropy', optimizer=optimizer)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "def save_imgs(epoch):\n    r, c = 5, 5\n    noise = np.random.normal(0, 1, (r * c, latent_dim))\n    gen_imgs = generator.predict(noise)\n\n    # Rescale images 0 - 1\n    gen_imgs = 0.5 * gen_imgs + 0.5\n\n    fig, axs = plt.subplots(r, c)\n    cnt = 0\n    for i in range(r):\n        for j in range(c):\n            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n            axs[i,j].axis('off')\n            cnt += 1\n    fig.savefig(\"images/mnist_%d.png\" % epoch)\n    plt.close()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "def train(epochs, batch_size=128, save_interval=50):\n\n    # Load the dataset\n    (X_train, _), (_, _) = mnist.load_data()\n\n    # Rescale -1 to 1\n    X_train = X_train / 127.5 - 1.\n    X_train = np.expand_dims(X_train, axis=3)\n\n    # Adversarial ground truths\n    valid = np.ones((batch_size, 1))\n    fake = np.zeros((batch_size, 1))\n\n    for epoch in range(epochs):\n\n        # ---------------------\n        #  Train Discriminator\n        # ---------------------\n\n        # Select a random half of images\n        idx = np.random.randint(0, X_train.shape[0], batch_size)\n        imgs = X_train[idx]\n\n        # Sample noise and generate a batch of new images\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n        gen_imgs = generator.predict(noise)\n\n        # Train the discriminator (real classified as ones and generated as zeros)\n        d_loss_real = discriminator.train_on_batch(imgs, valid)\n        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n        # ---------------------\n        #  Train Generator\n        # ---------------------\n\n        # Train the generator (wants discriminator to mistake images as real)\n        g_loss = combined.train_on_batch(noise, valid)\n\n        # Plot the progress\n        print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n\n        # If at save interval => save generated image samples\n        if epoch % save_interval == 0:\n            save_imgs(epoch)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "!mkdir -p images", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "train(epochs=4000, batch_size=32, save_interval=50)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "ls images", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from IPython.display import display\nfrom PIL import Image\n\n\npath=\"images/mnist_0.png\"\ndisplay(Image.open(path))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "\nfrom IPython.display import display\nfrom PIL import Image\n\n\npath=\"images/mnist_3950.png\"\ndisplay(Image.open(path))", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python36", "display_name": "Python 3.6 with Spark", "language": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.5.5"}}, "nbformat": 4, "nbformat_minor": 1}