{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on a [blog](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html) Francois Chollet has written mid 2016, please refer to that one of more information. In addition, I recommend reading [this](http://adventuresinmachinelearning.com/word2vec-keras-tutorial/) tutorial before to get started on word2vec\n",
    "\n",
    "\n",
    "\n",
    "First we'll download and exract the necessary files; the glove embeddings on the wikipedia corpus from [Stanford](https://nlp.stanford.edu/projects/glove/) as well as [newsgroup messages](http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.html) used for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove.6B.zip\n",
    "!wget http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\n",
    "!tar xvfz news20.tar.gz"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "COPYRIGHT\n",
    "\n",
    "All contributions by François Chollet:\n",
    "Copyright (c) 2015 - 2018, François Chollet.\n",
    "All rights reserved.\n",
    "\n",
    "All contributions by Google:\n",
    "Copyright (c) 2015 - 2018, Google, Inc.\n",
    "All rights reserved.\n",
    "\n",
    "All contributions by Microsoft:\n",
    "Copyright (c) 2017 - 2018, Microsoft, Inc.\n",
    "All rights reserved.\n",
    "\n",
    "All other contributions:\n",
    "Copyright (c) 2015 - 2018, the respective contributors.\n",
    "All rights reserved.\n",
    "\n",
    "Each contributor holds copyright over their respective contributions.\n",
    "The project versioning (Git) records all such contribution source information.\n",
    "\n",
    "LICENSE\n",
    "\n",
    "The MIT License (MIT)\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source taken from https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/fs01/user/se2e-1a118c4f322670-980ba6aaa6c3/.local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first, build index mapping words in the embeddings set to their embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join('glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "        \n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare text samples and their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "Found 19997 texts.\n"
     ]
    }
   ],
   "source": [
    "print('Processing text dataset')\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "for name in sorted(os.listdir('20_newsgroup')):\n",
    "    path = os.path.join('20_newsgroup', name)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)\n",
    "        labels_index[name] = label_id\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            if fname.isdigit():\n",
    "                fpath = os.path.join(path, fname)\n",
    "                args = {} if sys.version_info < (3,) else {'encoding': 'latin-1'}\n",
    "                with open(fpath, **args) as f:\n",
    "                    t = f.read()\n",
    "                    i = t.find('\\n\\n')  # skip header\n",
    "                    if 0 < i:\n",
    "                        t = t[i:]\n",
    "                    texts.append(t)\n",
    "                labels.append(label_id)\n",
    "\n",
    "print('Found %s texts.' % len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vectorize the text samples into a 2D integer tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 174105 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19997\n",
      "1528\n",
      "5116\n"
     ]
    }
   ],
   "source": [
    "print(len(sequences))\n",
    "print(len(sequences[0]))\n",
    "print(len(sequences[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (19997, 1000)\n",
      "Shape of label tensor: (19997, 20)\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split the data into a training set and a validation set , first, shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then split based on split percentage stored in VALIDATION_SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "#Untrained Embedding:\n",
    "#embedding_layer = Embedding(len(word_index) + 1,\n",
    "#                            EMBEDDING_DIM,\n",
    "#                            input_length=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n",
       "       -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n",
       "        0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n",
       "       -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n",
       "        0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n",
       "       -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n",
       "        0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n",
       "        0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n",
       "       -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n",
       "       -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n",
       "       -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n",
       "       -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n",
       "       -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n",
       "       -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n",
       "       -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n",
       "        0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n",
       "       -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index.get('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/30\n",
      "15998/15998 [==============================] - 81s 5ms/step - loss: 2.9310 - acc: 0.1970 - val_loss: 2.7595 - val_acc: 0.3701\n",
      "Epoch 2/30\n",
      "15998/15998 [==============================] - 82s 5ms/step - loss: 2.2948 - acc: 0.4995 - val_loss: 1.8832 - val_acc: 0.5699\n",
      "Epoch 3/30\n",
      "15998/15998 [==============================] - 81s 5ms/step - loss: 1.4982 - acc: 0.6572 - val_loss: 1.3274 - val_acc: 0.6569\n",
      "Epoch 4/30\n",
      "15998/15998 [==============================] - 80s 5ms/step - loss: 1.0370 - acc: 0.7355 - val_loss: 1.0437 - val_acc: 0.7034\n",
      "Epoch 5/30\n",
      "15998/15998 [==============================] - 81s 5ms/step - loss: 0.7835 - acc: 0.7878 - val_loss: 0.8983 - val_acc: 0.7344\n",
      "Epoch 6/30\n",
      "15998/15998 [==============================] - 76s 5ms/step - loss: 0.6242 - acc: 0.8295 - val_loss: 0.8134 - val_acc: 0.7539\n",
      "Epoch 7/30\n",
      "15998/15998 [==============================] - 72s 4ms/step - loss: 0.5095 - acc: 0.8579 - val_loss: 0.7555 - val_acc: 0.7699\n",
      "Epoch 8/30\n",
      "15998/15998 [==============================] - 74s 5ms/step - loss: 0.4232 - acc: 0.8815 - val_loss: 0.7231 - val_acc: 0.7722\n",
      "Epoch 9/30\n",
      "15998/15998 [==============================] - 84s 5ms/step - loss: 0.3553 - acc: 0.8992 - val_loss: 0.6949 - val_acc: 0.7824\n",
      "Epoch 10/30\n",
      "15998/15998 [==============================] - 78s 5ms/step - loss: 0.3001 - acc: 0.9159 - val_loss: 0.6829 - val_acc: 0.7917\n",
      "Epoch 11/30\n",
      "15998/15998 [==============================] - 78s 5ms/step - loss: 0.2548 - acc: 0.9247 - val_loss: 0.6690 - val_acc: 0.7949\n",
      "Epoch 12/30\n",
      "15998/15998 [==============================] - 80s 5ms/step - loss: 0.2173 - acc: 0.9350 - val_loss: 0.6732 - val_acc: 0.7934\n",
      "Epoch 13/30\n",
      "15998/15998 [==============================] - 79s 5ms/step - loss: 0.1877 - acc: 0.9433 - val_loss: 0.6694 - val_acc: 0.7974\n",
      "Epoch 14/30\n",
      "15998/15998 [==============================] - 79s 5ms/step - loss: 0.1629 - acc: 0.9506 - val_loss: 0.6788 - val_acc: 0.7949\n",
      "Epoch 15/30\n",
      "15998/15998 [==============================] - 79s 5ms/step - loss: 0.1418 - acc: 0.9556 - val_loss: 0.6975 - val_acc: 0.7919\n",
      "Epoch 16/30\n",
      "15998/15998 [==============================] - 79s 5ms/step - loss: 0.1256 - acc: 0.9604 - val_loss: 0.6932 - val_acc: 0.7982\n",
      "Epoch 17/30\n",
      "15998/15998 [==============================] - 79s 5ms/step - loss: 0.1109 - acc: 0.9637 - val_loss: 0.7044 - val_acc: 0.7984\n",
      "Epoch 18/30\n",
      "15998/15998 [==============================] - 79s 5ms/step - loss: 0.1013 - acc: 0.9663 - val_loss: 0.7176 - val_acc: 0.7984\n",
      "Epoch 19/30\n",
      "15998/15998 [==============================] - 79s 5ms/step - loss: 0.0921 - acc: 0.9668 - val_loss: 0.7280 - val_acc: 0.7957\n",
      "Epoch 20/30\n",
      "15998/15998 [==============================] - 79s 5ms/step - loss: 0.0851 - acc: 0.9678 - val_loss: 0.7596 - val_acc: 0.7947\n",
      "Epoch 21/30\n",
      "15998/15998 [==============================] - 79s 5ms/step - loss: 0.0789 - acc: 0.9682 - val_loss: 0.7625 - val_acc: 0.7977\n",
      "Epoch 22/30\n",
      "15998/15998 [==============================] - 79s 5ms/step - loss: 0.0743 - acc: 0.9691 - val_loss: 0.7618 - val_acc: 0.7984\n",
      "Epoch 23/30\n",
      "15998/15998 [==============================] - 81s 5ms/step - loss: 0.0700 - acc: 0.9699 - val_loss: 0.7972 - val_acc: 0.7977\n",
      "Epoch 24/30\n",
      "15998/15998 [==============================] - 89s 6ms/step - loss: 0.0675 - acc: 0.9701 - val_loss: 0.7991 - val_acc: 0.7982\n",
      "Epoch 25/30\n",
      "15998/15998 [==============================] - 92s 6ms/step - loss: 0.0646 - acc: 0.9700 - val_loss: 0.8121 - val_acc: 0.7989\n",
      "Epoch 26/30\n",
      "15998/15998 [==============================] - 90s 6ms/step - loss: 0.0628 - acc: 0.9710 - val_loss: 0.8266 - val_acc: 0.7982\n",
      "Epoch 27/30\n",
      "15998/15998 [==============================] - 89s 6ms/step - loss: 0.0607 - acc: 0.9701 - val_loss: 0.8435 - val_acc: 0.7972\n",
      "Epoch 28/30\n",
      "15998/15998 [==============================] - 94s 6ms/step - loss: 0.0595 - acc: 0.9709 - val_loss: 0.8533 - val_acc: 0.7999\n",
      "Epoch 29/30\n",
      "15998/15998 [==============================] - 92s 6ms/step - loss: 0.0587 - acc: 0.9707 - val_loss: 0.8695 - val_acc: 0.7949\n",
      "Epoch 30/30\n",
      "15998/15998 [==============================] - 81s 5ms/step - loss: 0.0571 - acc: 0.9701 - val_loss: 0.8643 - val_acc: 0.7974\n",
      "Trainin done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print('Training model.')\n",
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(embedded_sequences)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=30,\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "print(\"Training done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7974493623405852"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.argmax(preds,axis=1) == np.argmax(y_val,axis=1)) / float(len(preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print (\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print (\"Done.\",len(model),\" words loaded!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "model = loadGloveModel('glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.65520237287384"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concept_of_queen = model['king']-model['man']+model['woman']\n",
    "np.sum((concept_of_queen-model['queen'])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45.17898720772049"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concept_of_problems = model['iraq']-model['war']\n",
    "np.sum((concept_of_problems-model['iran'])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 with Spark",
   "language": "python3",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
