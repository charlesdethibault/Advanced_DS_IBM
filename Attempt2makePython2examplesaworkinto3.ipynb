{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Assignment 4\n## Understaning scaling of linear algebra operations on Apache Spark using Apache SystemML\n\nIn this assignment we want you to understand how to scale linear algebra operations from a single machine to multiple machines, memory and CPU cores using Apache SystemML. Therefore we want you to understand how to migrate from a numpy program to a SystemML DML program. Don't worry. We will give you a lot of hints. Finally, you won't need this knowledge anyways if you are sticking to Keras only, but once you go beyond that point you'll be happy to see what's going on behind the scenes. Please make sure you run this notebook from an Apache Spark 2.3 notebook.\n\nSo the first thing we need to ensure is that we are on the latest version of SystemML, which is 1.2.0:\n\nThe steps are:\n- pip install\n- link the jars to the correct location\n- restart the kernel\n- start execution at the cell with the version - check"}, {"metadata": {}, "cell_type": "code", "source": "!pip install systemml", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20191201150918-0001\nKERNEL_ID = cb5afeb0-4405-48e4-a799-66198bd91a1c\nCollecting systemml\nCollecting numpy>=1.8.2 (from systemml)\n  Using cached https://files.pythonhosted.org/packages/d2/ab/43e678759326f728de861edbef34b8e2ad1b1490505f20e0d1f0716c3bf4/numpy-1.17.4-cp36-cp36m-manylinux1_x86_64.whl\nCollecting pandas (from systemml)\n  Using cached https://files.pythonhosted.org/packages/52/3f/f6a428599e0d4497e1595030965b5ba455fd8ade6e977e3c819973c4b41d/pandas-0.25.3-cp36-cp36m-manylinux1_x86_64.whl\nCollecting scipy>=0.15.1 (from systemml)\n  Using cached https://files.pythonhosted.org/packages/54/18/d7c101d5e93b6c78dc206fcdf7bd04c1f8138a7b1a93578158fa3b132b08/scipy-1.3.3-cp36-cp36m-manylinux1_x86_64.whl\nCollecting scikit-learn (from systemml)\n  Using cached https://files.pythonhosted.org/packages/a0/c5/d2238762d780dde84a20b8c761f563fe882b88c5a5fb03c056547c442a19/scikit_learn-0.21.3-cp36-cp36m-manylinux1_x86_64.whl\nCollecting Pillow>=2.0.0 (from systemml)\n  Using cached https://files.pythonhosted.org/packages/10/5c/0e94e689de2476c4c5e644a3bd223a1c1b9e2bdb7c510191750be74fa786/Pillow-6.2.1-cp36-cp36m-manylinux1_x86_64.whl\nCollecting pytz>=2017.2 (from pandas->systemml)\n  Using cached https://files.pythonhosted.org/packages/e7/f9/f0b53f88060247251bf481fa6ea62cd0d25bf1b11a87888e53ce5b7c8ad2/pytz-2019.3-py2.py3-none-any.whl\nCollecting python-dateutil>=2.6.1 (from pandas->systemml)\n  Using cached https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl\nCollecting joblib>=0.11 (from scikit-learn->systemml)\n  Using cached https://files.pythonhosted.org/packages/8f/42/155696f85f344c066e17af287359c9786b436b1bf86029bb3411283274f3/joblib-0.14.0-py2.py3-none-any.whl\nCollecting six>=1.5 (from python-dateutil>=2.6.1->pandas->systemml)\n  Using cached https://files.pythonhosted.org/packages/65/26/32b8464df2a97e6dd1b656ed26b2c194606c16fe163c695a992b36c11cdf/six-1.13.0-py2.py3-none-any.whl\n\u001b[31mtensorflow 1.13.1 requires tensorboard<1.14.0,>=1.13.0, which is not installed.\u001b[0m\nInstalling collected packages: numpy, pytz, six, python-dateutil, pandas, scipy, joblib, scikit-learn, Pillow, systemml\nSuccessfully installed Pillow-6.2.1 joblib-0.14.0 numpy-1.17.4 pandas-0.25.3 python-dateutil-2.8.1 pytz-2019.3 scikit-learn-0.21.3 scipy-1.3.3 six-1.13.0 systemml-1.2.0\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/dateutil already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/Pillow-6.2.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/joblib already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/pytz already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/sklearn already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/six-1.13.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/pandas-0.25.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/PIL already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/systemml-1.2.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/numpy already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/scipy already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/joblib-0.14.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/python_dateutil-2.8.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/scikit_learn-0.21.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/pytz-2019.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/systemml already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/six.py already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/__pycache__ already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/numpy-1.17.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/pandas already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/scipy-1.3.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/bin already exists. Specify --upgrade to force replacement.\u001b[0m\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Now we need to create two sym links that the newest version is picket up - this is a workaround and will be removed soon"}, {"metadata": {}, "cell_type": "code", "source": "!ln -s -f ~/user-libs/python3.6/systemml/systemml-java/systemml-1.2.0-extra.jar ~/user-libs/spark2/systemml-1.2.0-extra.jar\n!ln -s -f ~/user-libs/python3.6/systemml/systemml-java/systemml-1.2.0.jar ~/user-libs/spark2/systemml-1.2.0.jar", "execution_count": 2, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Now please restart the kernel and make sure the version is correct"}, {"metadata": {}, "cell_type": "code", "source": "from systemml import MLContext\nml = MLContext(spark)\nprint(ml.version())\n    \nif not ml.version() == '1.2.0':\n    raise ValueError('please upgrade to SystemML 1.2.0, or restart your Kernel (Kernel->Restart & Clear Output)')", "execution_count": 3, "outputs": [{"output_type": "stream", "text": "1.2.0\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Congratulations, if you see version 1.2.0, please continue with the notebook..."}, {"metadata": {}, "cell_type": "code", "source": "from systemml import MLContext, dml\nimport numpy as np\nimport time", "execution_count": 4, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Then we create an MLContext to interface with Apache SystemML. Note that we pass a SparkSession object as parameter so SystemML now knows how to talk to the Apache Spark cluster"}, {"metadata": {}, "cell_type": "code", "source": "ml = MLContext(spark)", "execution_count": 5, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Now we create some large random matrices to have numpy and SystemML crunch on it"}, {"metadata": {}, "cell_type": "code", "source": "u = np.random.rand(1000,10000)\ns = np.random.rand(10000,1000)\nw = np.random.rand(1000,1000)", "execution_count": 6, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Now we implement a short one-liner to define a very simple linear algebra operation\n\nIn case you are unfamiliar with matrxi-matrix multiplication: https://en.wikipedia.org/wiki/Matrix_multiplication\n\nsum(U' * (W . (U * S)))\n\n\n| Legend        |            |   \n| ------------- |-------------| \n| '      | transpose of a matrix | \n| * | matrix-matrix multiplication      |  \n| . | scalar multiplication      |   \n\n"}, {"metadata": {}, "cell_type": "code", "source": "start = time.time()\nres = np.sum(u.T.dot(w * u.dot(s)))\nprint (time.time()-start)", "execution_count": 7, "outputs": [{"output_type": "stream", "text": "1.871474027633667\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "As you can see this executes perfectly fine. Note that this is even a very efficient execution because numpy uses a C/C++ backend which is known for it's performance. But what happens if U, S or W get such big that the available main memory cannot cope with it? Let's give it a try:"}, {"metadata": {}, "cell_type": "code", "source": "#u = np.random.rand(10000,100000)\n#s = np.random.rand(100000,10000)\n#w = np.random.rand(10000,10000)", "execution_count": 8, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# quiz week 2"}, {"metadata": {}, "cell_type": "code", "source": "from systemml import dml, MLContext\nimport numpy as np\n", "execution_count": 13, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "ml = MLContext(spark)", "execution_count": 14, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "script=\"\"\"\nc = sum(a %*% t(b))\n\"\"\"", "execution_count": 15, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "a=np.array([[1,2,3]])\nb=np.array([[4,5,6]])\n\nprog = dml(script).input('a',a).input('b',b).output('c')\nc = ml.execute(prog).get('c')", "execution_count": 17, "outputs": [{"output_type": "stream", "text": "SystemML Statistics:\nTotal execution time:\t\t0.012 sec.\nNumber of executed Spark inst:\t2.\n\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "c", "execution_count": 19, "outputs": [{"output_type": "execute_result", "execution_count": 19, "data": {"text/plain": "32.0"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "np.sum(a.dot(b.T))", "execution_count": 18, "outputs": [{"output_type": "execute_result", "execution_count": 18, "data": {"text/plain": "32"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python36", "display_name": "Python 3.6 with Spark", "language": "python3"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "pygments_lexer": "ipython3", "version": "3.6.8", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4, "nbformat_minor": 1}