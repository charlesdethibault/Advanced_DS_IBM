{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for a Spark session to start...\n",
      "Spark Initialization Done! ApplicationId = app-20191210194609-0000\n",
      "KERNEL_ID = 79f875b0-d0f1-49b9-9faf-ae146132517b\n",
      "--2019-12-10 19:46:11--  https://github.com/maxpumperla/dl4j_coursera/releases/download/v0.4/dl4j-snapshot.jar\n",
      "Resolving github.com (github.com)... 140.82.114.4\n",
      "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/113966420/3472050e-f84b-11e7-90f0-d69fe0bedce0?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191210%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20191210T194612Z&X-Amz-Expires=300&X-Amz-Signature=6b8217424c457d83375e707c747caf1f95a2ecdef6f75d562fa9f1f2eb08749b&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Ddl4j-snapshot.jar&response-content-type=application%2Foctet-stream [following]\n",
      "--2019-12-10 19:46:12--  https://github-production-release-asset-2e65be.s3.amazonaws.com/113966420/3472050e-f84b-11e7-90f0-d69fe0bedce0?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191210%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20191210T194612Z&X-Amz-Expires=300&X-Amz-Signature=6b8217424c457d83375e707c747caf1f95a2ecdef6f75d562fa9f1f2eb08749b&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Ddl4j-snapshot.jar&response-content-type=application%2Foctet-stream\n",
      "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.217.37.156\n",
      "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.217.37.156|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 486534267 (464M) [application/octet-stream]\n",
      "Saving to: 'dl4j-snapshot.jar'\n",
      "\n",
      "100%[======================================>] 486,534,267 57.0MB/s   in 8.6s   \n",
      "\n",
      "2019-12-10 19:46:21 (53.8 MB/s) - 'dl4j-snapshot.jar' saved [486534267/486534267]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/maxpumperla/dl4j_coursera/releases/download/v0.4/dl4j-snapshot.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-12-10 19:46:21--  https://raw.githubusercontent.com/maxpumperla/dl4j_coursera/master/iris.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 199.232.8.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.232.8.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2850 (2.8K) [text/plain]\n",
      "Saving to: 'iris.txt'\n",
      "\n",
      "100%[======================================>] 2,850       --.-K/s   in 0s      \n",
      "\n",
      "2019-12-10 19:46:22 (22.9 MB/s) - 'iris.txt' saved [2850/2850]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/maxpumperla/dl4j_coursera/master/iris.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 10\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"iris.txt\", header=None)\n",
    "dataset = dataframe.values\n",
    "X = dataset[:,0:4].astype(float)\n",
    "Y = dataset[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/ibm/conda/miniconda3.6/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/ibm/conda/miniconda3.6/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 1.5465 - acc: 0.3333\n",
      "Epoch 2/20\n",
      "150/150 [==============================] - 0s 414us/step - loss: 1.4430 - acc: 0.3333\n",
      "Epoch 3/20\n",
      "150/150 [==============================] - 0s 303us/step - loss: 1.3724 - acc: 0.3333\n",
      "Epoch 4/20\n",
      "150/150 [==============================] - 0s 395us/step - loss: 1.3158 - acc: 0.3333\n",
      "Epoch 5/20\n",
      "150/150 [==============================] - 0s 307us/step - loss: 1.2737 - acc: 0.3333\n",
      "Epoch 6/20\n",
      "150/150 [==============================] - 0s 351us/step - loss: 1.2401 - acc: 0.3333\n",
      "Epoch 7/20\n",
      "150/150 [==============================] - 0s 311us/step - loss: 1.2116 - acc: 0.3333\n",
      "Epoch 8/20\n",
      "150/150 [==============================] - 0s 297us/step - loss: 1.1896 - acc: 0.3333\n",
      "Epoch 9/20\n",
      "150/150 [==============================] - 0s 277us/step - loss: 1.1705 - acc: 0.3333\n",
      "Epoch 10/20\n",
      "150/150 [==============================] - 0s 279us/step - loss: 1.1535 - acc: 0.3333\n",
      "Epoch 11/20\n",
      "150/150 [==============================] - 0s 766us/step - loss: 1.1395 - acc: 0.3333\n",
      "Epoch 12/20\n",
      "150/150 [==============================] - 0s 535us/step - loss: 1.1289 - acc: 0.4400\n",
      "Epoch 13/20\n",
      "150/150 [==============================] - 0s 361us/step - loss: 1.1183 - acc: 0.5333\n",
      "Epoch 14/20\n",
      "150/150 [==============================] - 0s 268us/step - loss: 1.1118 - acc: 0.3667\n",
      "Epoch 15/20\n",
      "150/150 [==============================] - 0s 309us/step - loss: 1.1078 - acc: 0.3333\n",
      "Epoch 16/20\n",
      "150/150 [==============================] - 0s 309us/step - loss: 1.1045 - acc: 0.3333\n",
      "Epoch 17/20\n",
      "150/150 [==============================] - 0s 581us/step - loss: 1.1016 - acc: 0.3333\n",
      "Epoch 18/20\n",
      "150/150 [==============================] - 0s 284us/step - loss: 1.0994 - acc: 0.3333\n",
      "Epoch 19/20\n",
      "150/150 [==============================] - 0s 293us/step - loss: 1.0978 - acc: 0.3333\n",
      "Epoch 20/20\n",
      "150/150 [==============================] - 0s 297us/step - loss: 1.0986 - acc: 0.3333\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(4, input_dim=4, activation='relu'))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X,dummy_y,epochs=20,batch_size=5)\n",
    "model.save('iris_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some learners constantly reported 502 errors in Watson Studio. \n",
    "#This is due to the limited resources in the free tier and the heavy resource consumption of Keras.\n",
    "#This is a workaround to limit resource consumption\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "K.set_session(K.tf.Session(config=K.tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Unrecognized option: -batchSizePerWorker\n",
      "\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/opt/ibm/spark/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/opt/ibm/image-libs/spark2/tika-app-1.14.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "Usage: spark-submit [options] <app jar | python file | R file> [app arguments]\n",
      "Usage: spark-submit --kill [submission ID] --master [spark://...]\n",
      "Usage: spark-submit --status [submission ID] --master [spark://...]\n",
      "Usage: spark-submit run-example [options] example-class [example args]\n",
      "\n",
      "Options:\n",
      "  --master MASTER_URL         spark://host:port, mesos://host:port, yarn,\n",
      "                              k8s://https://host:port, or local (Default: local[*]).\n",
      "  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (\"client\") or\n",
      "                              on one of the worker machines inside the cluster (\"cluster\")\n",
      "                              (Default: client).\n",
      "  --class CLASS_NAME          Your application's main class (for Java / Scala apps).\n",
      "  --name NAME                 A name of your application.\n",
      "  --jars JARS                 Comma-separated list of jars to include on the driver\n",
      "                              and executor classpaths.\n",
      "  --packages                  Comma-separated list of maven coordinates of jars to include\n",
      "                              on the driver and executor classpaths. Will search the local\n",
      "                              maven repo, then maven central and any additional remote\n",
      "                              repositories given by --repositories. The format for the\n",
      "                              coordinates should be groupId:artifactId:version.\n",
      "  --exclude-packages          Comma-separated list of groupId:artifactId, to exclude while\n",
      "                              resolving the dependencies provided in --packages to avoid\n",
      "                              dependency conflicts.\n",
      "  --repositories              Comma-separated list of additional remote repositories to\n",
      "                              search for the maven coordinates given with --packages.\n",
      "  --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to place\n",
      "                              on the PYTHONPATH for Python apps.\n",
      "  --files FILES               Comma-separated list of files to be placed in the working\n",
      "                              directory of each executor. File paths of these files\n",
      "                              in executors can be accessed via SparkFiles.get(fileName).\n",
      "\n",
      "  --conf PROP=VALUE           Arbitrary Spark configuration property.\n",
      "  --properties-file FILE      Path to a file from which to load extra properties. If not\n",
      "                              specified, this will look for conf/spark-defaults.conf.\n",
      "\n",
      "  --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).\n",
      "  --driver-java-options       Extra Java options to pass to the driver.\n",
      "  --driver-library-path       Extra library path entries to pass to the driver.\n",
      "  --driver-class-path         Extra class path entries to pass to the driver. Note that\n",
      "                              jars added with --jars are automatically included in the\n",
      "                              classpath.\n",
      "\n",
      "  --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).\n",
      "\n",
      "  --proxy-user NAME           User to impersonate when submitting the application.\n",
      "                              This argument does not work with --principal / --keytab.\n",
      "\n",
      "  --help, -h                  Show this help message and exit.\n",
      "  --verbose, -v               Print additional debug output.\n",
      "  --version,                  Print the version of current Spark.\n",
      "\n",
      " Cluster deploy mode only:\n",
      "  --driver-cores NUM          Number of cores used by the driver, only in cluster mode\n",
      "                              (Default: 1).\n",
      "\n",
      " Spark standalone or Mesos with cluster deploy mode only:\n",
      "  --supervise                 If given, restarts the driver on failure.\n",
      "  --kill SUBMISSION_ID        If given, kills the driver specified.\n",
      "  --status SUBMISSION_ID      If given, requests the status of the driver specified.\n",
      "\n",
      " Spark standalone and Mesos only:\n",
      "  --total-executor-cores NUM  Total cores for all executors.\n",
      "\n",
      " Spark standalone and YARN only:\n",
      "  --executor-cores NUM        Number of cores per executor. (Default: 1 in YARN mode,\n",
      "                              or all available cores on the worker in standalone mode)\n",
      "\n",
      " YARN-only:\n",
      "  --queue QUEUE_NAME          The YARN queue to submit to (Default: \"default\").\n",
      "  --num-executors NUM         Number of executors to launch (Default: 2).\n",
      "                              If dynamic allocation is enabled, the initial number of\n",
      "                              executors will be at least NUM.\n",
      "  --archives ARCHIVES         Comma separated list of archives to be extracted into the\n",
      "                              working directory of each executor.\n",
      "  --principal PRINCIPAL       Principal to be used to login to KDC, while running on\n",
      "                              secure HDFS.\n",
      "  --keytab KEYTAB             The full path to the file that contains the keytab for the\n",
      "                              principal specified above. This keytab will be copied to\n",
      "                              the node running the Application Master via the Secure\n",
      "                              Distributed Cache, for renewing the login tickets and the\n",
      "                              delegation tokens periodically.\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "!$SPARK_HOME/bin/spark-submit \\\n",
    "--class skymind.dsx.KerasImportCSVSparkRunner \\\n",
    "--files iris.txt,iris_model.h5 \\\n",
    "--master $MASTER \\\n",
    "dl4j-snapshot.jar \\\n",
    "-batchSizePerWorker 15 \\\n",
    "-indexLabel 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat 'dl4j-snapshot.jar.1': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!mv dl4j-snapshot.jar.1 dl4-snapshot.jar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dl4j-snapshot.jar\r\n"
     ]
    }
   ],
   "source": [
    "!ls dl4j-snapshot.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 with Spark",
   "language": "python3",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
